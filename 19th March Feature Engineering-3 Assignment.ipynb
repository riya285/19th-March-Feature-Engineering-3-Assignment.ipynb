{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57671573-a507-4dcd-8d00-f73871a3c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features in a dataset to a specific range, usually between 0 and 1. This method is particularly useful when the features have different scales or ranges and need to be brought to a common scale. Min-Max scaling works by subtracting the minimum value of the feature and then dividing by the range (difference between maximum and minimum values).\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "�\n",
    "scaled\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "X is the original value of the feature.\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value of the feature in the dataset.\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value of the feature in the dataset.\n",
    "�\n",
    "scaled\n",
    "X \n",
    "scaled\n",
    "​\n",
    "  is the scaled value of the feature.\n",
    "Here's an example to illustrate its application:\n",
    "\n",
    "Suppose you have a dataset with a feature \"Age\" representing people's ages, and the ages range from 18 to 80. You want to apply Min-Max scaling to bring these ages into the range of 0 to 1.\n",
    "\n",
    "Original Age values: [18, 25, 40, 60, 80]\n",
    "\n",
    "Find the minimum and maximum values:\n",
    "\n",
    "�\n",
    "min\n",
    "=\n",
    "18\n",
    "X \n",
    "min\n",
    "​\n",
    " =18\n",
    "�\n",
    "max\n",
    "=\n",
    "80\n",
    "X \n",
    "max\n",
    "​\n",
    " =80\n",
    "Apply the Min-Max scaling formula to each value:\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "18\n",
    "X=18:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "18\n",
    "−\n",
    "18\n",
    "80\n",
    "−\n",
    "18\n",
    "=\n",
    "0\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "80−18\n",
    "18−18\n",
    "​\n",
    " =0\n",
    "For \n",
    "�\n",
    "=\n",
    "25\n",
    "X=25:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "25\n",
    "−\n",
    "18\n",
    "80\n",
    "−\n",
    "18\n",
    "≈\n",
    "0.0667\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "80−18\n",
    "25−18\n",
    "​\n",
    " ≈0.0667\n",
    "For \n",
    "�\n",
    "=\n",
    "40\n",
    "X=40:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "40\n",
    "−\n",
    "18\n",
    "80\n",
    "−\n",
    "18\n",
    "≈\n",
    "0.4\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "80−18\n",
    "40−18\n",
    "​\n",
    " ≈0.4\n",
    "For \n",
    "�\n",
    "=\n",
    "60\n",
    "X=60:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "60\n",
    "−\n",
    "18\n",
    "80\n",
    "−\n",
    "18\n",
    "≈\n",
    "0.8\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "80−18\n",
    "60−18\n",
    "​\n",
    " ≈0.8\n",
    "For \n",
    "�\n",
    "=\n",
    "80\n",
    "X=80:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "80\n",
    "−\n",
    "18\n",
    "80\n",
    "−\n",
    "18\n",
    "=\n",
    "1\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "80−18\n",
    "80−18\n",
    "​\n",
    " =1\n",
    "After Min-Max scaling, the scaled Age values will be approximately:\n",
    "[\n",
    "0\n",
    ",\n",
    "0.0667\n",
    ",\n",
    "0.4\n",
    ",\n",
    "0.8\n",
    ",\n",
    "1\n",
    "]\n",
    "[0,0.0667,0.4,0.8,1]\n",
    "\n",
    "Now, all the age values are within the range of 0 to 1, which can help algorithms that are sensitive to feature scales perform better and more accurately.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The Unit Vector technique, also known as Vector Normalization or L2 Normalization, is a feature scaling method used to transform data in a way that each sample (data point) has a unit norm, i.e., a length of 1, in the feature space. This technique is particularly useful when the magnitude of individual features is not as important as their direction or relative relationships.\n",
    "\n",
    "The Unit Vector technique involves dividing each data point by its Euclidean norm (also known as the L2 norm). Mathematically, for a data point represented as a vector x = [x1, x2, ..., xn], the normalized vector x_norm is calculated as:\n",
    "\n",
    "x_norm = x / ||x||,\n",
    "\n",
    "where ||x|| represents the Euclidean norm of vector x.\n",
    "\n",
    "In contrast, Min-Max scaling is another feature scaling technique that transforms the data to a specific range, usually between 0 and 1. It subtracts the minimum value of the feature and then divides by the range (difference between maximum and minimum values).\n",
    "\n",
    "Here's a comparison using a simple example to illustrate the differences:\n",
    "\n",
    "Suppose you have a dataset with two features, \"Age\" and \"Income,\" and you want to scale them using both the Unit Vector technique and Min-Max scaling. Let's assume the following data:\n",
    "\n",
    "\n",
    "|  Age  |  Income  |\n",
    "|-------|----------|\n",
    "|  30   |  50000   |\n",
    "|  40   |  60000   |\n",
    "|  25   |  70000   |\n",
    "\n",
    "\n",
    "Unit Vector Technique (L2 Normalization):\n",
    "Calculate the Euclidean norm of each data point:\n",
    "\n",
    "For (30, 50000): ||(30, 50000)|| = √(30^2 + 50000^2) ≈ 50015.3\n",
    "For (40, 60000): ||(40, 60000)|| = √(40^2 + 60000^2) ≈ 60000.1\n",
    "For (25, 70000): ||(25, 70000)|| = √(25^2 + 70000^2) ≈ 70000.2\n",
    "Normalized data:\n",
    "\n",
    "(30, 50000) → (30 / 50015.3, 50000 / 50015.3) ≈ (0.0005998, 0.9999998)\n",
    "(40, 60000) → (40 / 60000.1, 60000 / 60000.1) ≈ (0.0006667, 0.9999999)\n",
    "(25, 70000) → (25 / 70000.2, 70000 / 70000.2) ≈ (0.0003571, 1.0)\n",
    "Min-Max Scaling:\n",
    "For Age:\n",
    "\n",
    "Min: 25\n",
    "Max: 40\n",
    "Range: 40 - 25 = 15\n",
    "For Income:\n",
    "\n",
    "Min: 50000\n",
    "Max: 70000\n",
    "Range: 70000 - 50000 = 20000\n",
    "Normalized data:\n",
    "\n",
    "(30, 50000) → ((30 - 25) / 15, (50000 - 50000) / 20000) = (0.3333, 0.0)\n",
    "(40, 60000) → ((40 - 25) / 15, (60000 - 50000) / 20000) = (1.0, 0.05)\n",
    "(25, 70000) → ((25 - 25) / 15, (70000 - 50000) / 20000) = (0.0, 1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in statistics and machine learning to simplify the complexity of high-dimensional data while retaining as much of the original variability as possible. It does this by transforming the data into a new coordinate system defined by its principal components, which are orthogonal directions that capture the most variance in the data.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "Data Standardization: If the features in the dataset are on different scales, it's important to standardize them to have mean 0 and standard deviation 1. This step ensures that all features contribute equally to the PCA process.\n",
    "\n",
    "Calculate Covariance Matrix: The covariance matrix is calculated for the standardized data. The covariance between two features measures how they vary together. A higher covariance suggests that changes in one feature are related to changes in another.\n",
    "\n",
    "Calculate Eigenvectors and Eigenvalues: The eigenvectors and eigenvalues of the covariance matrix are computed. Eigenvectors are the directions along which the data varies the most, and eigenvalues indicate the variance of the data in those directions.\n",
    "\n",
    "Select Principal Components: The eigenvectors are sorted based on their corresponding eigenvalues in decreasing order. The eigenvectors with the highest eigenvalues are the principal components (PCs). These are the most significant directions of variation in the data.\n",
    "\n",
    "Projection onto New Space: The original data is projected onto the new coordinate system defined by the selected principal components. This results in a reduced-dimensional representation of the data.\n",
    "\n",
    "PCA is often used for dimensionality reduction because it can help remove noise, reduce computational complexity, and potentially improve the performance of machine learning algorithms that suffer from the curse of dimensionality. It is important to note that while PCA reduces dimensions, it doesn't always guarantee interpretability of the transformed features.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a dataset with two features, \"Height\" (in inches) and \"Weight\" (in pounds), and we have measurements for several individuals. The data might look something like this:\n",
    "\n",
    "Individual\tHeight (in)\t             Weight (lbs)\n",
    "1\t            65\t                    150\n",
    "2\t            70\t                    180\n",
    "3\t            63                      135\n",
    "...\t...\t...\n",
    "In this example, we can apply PCA to perform dimensionality reduction. Let's say we're interested in reducing the data to a single dimension (1D PCA):\n",
    "\n",
    "Standardize the data (subtract mean, divide by standard deviation).\n",
    "Calculate the covariance matrix.\n",
    "Calculate the eigenvector and eigenvalue.\n",
    "Select the first principal component (highest eigenvalue).\n",
    "Project the data onto the first principal component.\n",
    "The result of this projection would give us a new 1D representation of the data. This new dimension would capture the most significant variation in the original data, which might be a combination of height and weight, but with less complexity. The reduced-dimensional data could then be used for further analysis or modeling.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that is often used for feature extraction in machine learning and data analysis. It's a mathematical procedure that transforms the original features of a dataset into a new set of features called principal components, which are linear combinations of the original features. These new features are ordered by the amount of variance they capture in the data, with the first principal component capturing the most variance, the second capturing the second most, and so on.\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA can be used to extract a reduced set of informative features from a high-dimensional dataset. This is particularly useful for simplifying the data and reducing noise, while retaining as much of the relevant information as possible.\n",
    "\n",
    "Here's a step-by-step explanation of how PCA can be used for feature extraction:\n",
    "\n",
    "Standardize the Data: Before applying PCA, it's common practice to standardize the data by subtracting the mean and dividing by the standard deviation. This ensures that all features have the same scale, which is important for the PCA computation.\n",
    "\n",
    "Calculate Covariance Matrix: PCA operates on the covariance matrix of the standardized data. The covariance matrix captures the relationships between different features, indicating how they vary together.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: The next step involves calculating the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions of maximum variance in the data, and eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "Sort Eigenvectors: Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvector with the highest eigenvalue (largest variance) becomes the first principal component, and so on.\n",
    "\n",
    "Select Principal Components: You can select a subset of the top principal components based on the amount of variance you want to retain. For example, if you choose to retain 95% of the variance, you would select the first k principal components that cumulatively explain at least 95% of the total variance.\n",
    "\n",
    "Transform Data: Finally, you can transform the original data into the reduced feature space by projecting it onto the selected principal components.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say you have a dataset of images with 1000 pixel features each (1000 dimensions). You want to reduce the dimensionality while retaining as much visual information as possible. You apply PCA to this dataset:\n",
    "\n",
    "Standardize the pixel values.\n",
    "Calculate the covariance matrix.\n",
    "Compute eigenvectors and eigenvalues.\n",
    "Sort eigenvectors by eigenvalues.\n",
    "Select the top k eigenvectors (principal components) to retain, let's say k = 50.\n",
    "You have now reduced the dimensionality from 1000 features to 50 principal components. Each image can be represented by these 50 principal components, which capture the most important patterns and variations in the data. This representation is a form of feature extraction, as you've transformed the original pixel features into a more compact and informative set of features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to scale numerical features within a specific range, usually between 0 and 1. This technique is commonly employed when features have different scales, and you want to ensure that all features contribute equally to the analysis or modeling process. In the context of building a recommendation system for a food delivery service using features like price, rating, and delivery time, here's how you could use Min-Max scaling:\n",
    "\n",
    "Understand the Data:\n",
    "Before applying any preprocessing technique, it's important to understand the characteristics of your data. For each feature (price, rating, delivery time), consider the range of values they can take, their distribution, and potential outliers.\n",
    "\n",
    "Determine the Scaling Range:\n",
    "Decide on the scaling range you want to use. The most common choice is to scale the features between 0 and 1. However, you might choose a different range depending on your specific requirements and the nature of the data.\n",
    "\n",
    "Calculate Min-Max Scaling:\n",
    "For each numerical feature, follow these steps to perform Min-Max scaling:\n",
    "\n",
    "a. Calculate the minimum (min_val) and maximum (max_val) values for the feature within your dataset.\n",
    "\n",
    "b. For each data point, apply the Min-Max scaling formula:\n",
    "\n",
    "    \n",
    "\n",
    "scaled_value = (original_value - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "This formula scales the original value to a value between 0 and 1 based on its relative position within the minimum and maximum values of the feature.\n",
    "\n",
    "Implement the Scaling:\n",
    "Depending on your programming environment (Python, R, etc.), you can implement the Min-Max scaling using libraries like scikit-learn in Python or equivalent libraries in other languages. These libraries offer pre-built functions to easily perform scaling.\n",
    "\n",
    "Here's a Python example using scikit-learn:\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming 'data' is your dataset containing features like price, rating, and delivery time\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "Interpretation:\n",
    "After applying Min-Max scaling, your features will be transformed to values between 0 and 1. This ensures that no feature dominates others due to differences in scale. The scaled features are now ready to be used as inputs for your recommendation system.\n",
    "\n",
    "Considerations:\n",
    "\n",
    "Min-Max scaling is sensitive to outliers. If your data contains extreme outliers, their influence might be amplified after scaling. You might need to consider outlier handling techniques before applying scaling.\n",
    "Always scale your training data and use the same scaling parameters for your testing/validation data to ensure consistency.\n",
    "\n",
    "\n",
    "\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis to transform high-dimensional data into a lower-dimensional representation while preserving as much of the original data's variance as possible. In the context of your project to predict stock prices using a dataset with numerous features, PCA can be a valuable tool to simplify the dataset and improve the efficiency of your model.\n",
    "\n",
    "Here's how you would use PCA to reduce the dimensionality of your stock price prediction dataset:\n",
    "\n",
    "Data Preprocessing: Start by ensuring that your data is properly preprocessed. This involves handling missing values, normalizing or standardizing features to ensure they have comparable scales, and preparing the dataset for analysis.\n",
    "\n",
    "Calculate the Covariance Matrix: PCA relies on the covariance matrix of the features in your dataset. Calculate the covariance matrix to understand the relationships and dependencies between the features.\n",
    "\n",
    "Eigenvalue Decomposition: Perform eigenvalue decomposition on the covariance matrix to extract its eigenvectors and eigenvalues. Eigenvectors represent the directions of maximum variance in the original feature space, while eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "Sort and Select Components: Sort the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues capture the most significant variance in the data. You'll need to decide how many principal components (PCs) to retain. This decision involves trading off between dimensionality reduction and the amount of variance you're willing to preserve.\n",
    "\n",
    "Dimensionality Reduction: Project the original data onto the selected eigenvectors (principal components). These new dimensions are linear combinations of the original features. By choosing a subset of the principal components, you effectively reduce the dimensionality of the dataset.\n",
    "\n",
    "Creating the Reduced Dataset: The new dataset is created by stacking the selected principal components as columns. This reduced-dimensional dataset retains the most important information from the original dataset while discarding less important dimensions.\n",
    "\n",
    "Model Training and Evaluation: With the reduced-dimensional dataset, you can now train your stock price prediction model. The reduced dimensionality often results in faster training times and may mitigate overfitting, especially when dealing with high-dimensional data.\n",
    "\n",
    "Inversion (if needed): If necessary, you can use the inverse transformation to map predictions made on the reduced dataset back to the original feature space for interpretability.\n",
    "\n",
    "It's important to note that while PCA can effectively reduce dimensionality, it might not always lead to improved predictive performance. Some information might be lost in the process, particularly if you choose to retain only a small number of principal components. It's a trade-off between simplifying the data and retaining enough information for accurate predictions.\n",
    "\n",
    "Additionally, in financial markets, the assumptions underlying PCA (such as linear relationships) might not always hold true due to the complex and nonlinear nature of market behavior. Therefore, it's recommended to experiment with various dimensionality reduction techniques and evaluate their impact on your specific stock price prediction task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\n",
    "\n",
    "\n",
    "Min-Max scaling is a common technique used to transform numerical data to a specific range, often between 0 and 1 or -1 and 1. To perform Min-Max scaling on your dataset, which has values [1, 5, 10, 15, 20], to the range of -1 to 1, you can use the following formula:\n",
    "\n",
    "Scaled Value = (X - X_min) / (X_max - X_min) * 2 - 1\n",
    "\n",
    "Where:\n",
    "\n",
    "X is the original value.\n",
    "X_min is the minimum value in the dataset.\n",
    "X_max is the maximum value in the dataset.\n",
    "Let's calculate the scaled values:\n",
    "\n",
    "Find X_min and X_max:\n",
    "X_min = 1 (minimum value)\n",
    "X_max = 20 (maximum value)\n",
    "\n",
    "Calculate the scaled values using the formula for each value in the dataset:\n",
    "\n",
    "For X = 1:\n",
    "Scaled Value = (1 - 1) / (20 - 1) * 2 - 1 = 0 / 19 * 2 - 1 = -1\n",
    "\n",
    "For X = 5:\n",
    "Scaled Value = (5 - 1) / (20 - 1) * 2 - 1 = 4 / 19 * 2 - 1 = -0.631578947\n",
    "\n",
    "For X = 10:\n",
    "Scaled Value = (10 - 1) / (20 - 1) * 2 - 1 = 9 / 19 * 2 - 1 = -0.157894737\n",
    "\n",
    "For X = 15:\n",
    "Scaled Value = (15 - 1) / (20 - 1) * 2 - 1 = 14 / 19 * 2 - 1 = 0.315789474\n",
    "\n",
    "For X = 20:\n",
    "Scaled Value = (20 - 1) / (20 - 1) * 2 - 1 = 19 / 19 * 2 - 1 = 1\n",
    "\n",
    "So, the Min-Max scaled values for the given dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are approximately:\n",
    "[-1, -0.6316, -0.1579, 0.3158, 1]\n",
    "\n",
    "\n",
    "\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "\n",
    "\n",
    "The decision of how many principal components to retain in a PCA (Principal Component Analysis) depends on the variance explained by each component and the desired level of dimensionality reduction. The goal is to strike a balance between reducing dimensionality while retaining as much information as possible.\n",
    "\n",
    "Here are the steps you can follow to determine the number of principal components to retain:\n",
    "\n",
    "Calculate the Covariance Matrix: Start by calculating the covariance matrix of your dataset's features.\n",
    "\n",
    "Calculate Eigenvalues and Eigenvectors: Compute the eigenvalues and eigenvectors of the covariance matrix. These eigenvalues represent the amount of variance captured by each corresponding eigenvector (principal component).\n",
    "\n",
    "Sort Eigenvalues: Sort the eigenvalues in descending order. This will give you an idea of how much variance each principal component captures.\n",
    "\n",
    "Explained Variance: Calculate the explained variance ratio for each principal component. This can be done by dividing each eigenvalue by the sum of all eigenvalues. The explained variance ratio indicates the proportion of total variance explained by each principal component.\n",
    "\n",
    "Cumulative Explained Variance: Calculate the cumulative explained variance by summing up the explained variance ratios for each principal component. This will help you understand how much total variance is captured by retaining a certain number of components.\n",
    "\n",
    "Elbow Method and Scree Plot: Plot the explained variance ratios against the number of principal components. Look for an \"elbow point\" on the plot where the explained variance starts to level off. This point often represents a good trade-off between dimensionality reduction and retaining information.\n",
    "\n",
    "Select Number of Components: Choose the number of principal components that correspond to the elbow point or where the cumulative explained variance reaches a satisfactory level. Generally, a common rule of thumb is to retain components that explain around 95% of the total variance.\n",
    "\n",
    "Project Data: Finally, you can project your original data onto the selected principal components to reduce dimensionality.\n",
    "\n",
    "Without having the actual data and its characteristics, it's challenging to give a specific number of principal components to retain. However, a reasonable approach would involve creating a scree plot and examining the explained variance to decide on the appropriate number of components that captures a satisfactory amount of the total variance while reducing dimensionality.\n",
    "\n",
    "Remember that retaining too few components may result in loss of information, while retaining too many components might defeat the purpose of dimensionality reduction. It's a trade-off that depends on the specific context and goals of your analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
